{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_EL4106.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO0xSaBUW5E3LyJmy+u3NBh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiegoPincheiraIb/EL4106_Assignments/blob/main/Assignment5/Assignment5_EL4106.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STkot7H1C3kI"
      },
      "source": [
        "# Configuraciones preliminares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWuhqz-Ua04o"
      },
      "source": [
        "## Importaci贸n de librerias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1XcGOgTa0LG"
      },
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7o5-77YrvH2"
      },
      "source": [
        "## Importaci贸n de archivos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOVSuTFwe50Q"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/DiegoPincheiraIb/EL4106_Assignments/main/Assignment5/Data_Cortex_Nuclear.xls'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjJTGvg-rxKG"
      },
      "source": [
        "# Procesamiento de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56A7XuPGidLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9712630e-9140-4323-d05e-db2da771a429"
      },
      "source": [
        "Datos_Iniciales = pd.read_excel(url)\n",
        "## ---- Procesamiento de Dataframe original\n",
        "del Datos_Iniciales['MouseID'], Datos_Iniciales['Behavior'], Datos_Iniciales['Genotype'], Datos_Iniciales['Treatment'], Datos_Iniciales['BCL2_N']\n",
        "# Se eliminan las filas con datos NaN\n",
        "Datos_Iniciales_Copia = Datos_Iniciales.copy()\n",
        "Datos_Iniciales = Datos_Iniciales_Copia.dropna()\n",
        "print(Datos_Iniciales)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      DYRK1A_N   ITSN1_N    BDNF_N  ...  H3MeK4_N    CaNA_N   class\n",
            "0     0.503644  0.747193  0.430175  ...  0.128186  1.675652  c-CS-m\n",
            "1     0.514617  0.689064  0.411770  ...  0.131119  1.743610  c-CS-m\n",
            "2     0.509183  0.730247  0.418309  ...  0.127431  1.926427  c-CS-m\n",
            "3     0.442107  0.617076  0.358626  ...  0.146901  1.700563  c-CS-m\n",
            "4     0.434940  0.617430  0.358802  ...  0.148380  1.839730  c-CS-m\n",
            "...        ...       ...       ...  ...       ...       ...     ...\n",
            "1075  0.254860  0.463591  0.254860  ...  0.328327  1.364823  t-SC-s\n",
            "1076  0.272198  0.474163  0.251638  ...  0.293435  1.364478  t-SC-s\n",
            "1077  0.228700  0.395179  0.234118  ...  0.355213  1.430825  t-SC-s\n",
            "1078  0.221242  0.412894  0.243974  ...  0.365353  1.404031  t-SC-s\n",
            "1079  0.302626  0.461059  0.256564  ...  0.365278  1.370999  t-SC-s\n",
            "\n",
            "[612 rows x 77 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA3WTsEvucz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec86e003-938c-4e43-8101-3ac586b76063"
      },
      "source": [
        "X_datos = Datos_Iniciales.iloc[:,:-1].values\n",
        "Y_datos = Datos_Iniciales.iloc[:,-1].values\n",
        "data = X_datos\n",
        "y_digits = Y_datos\n",
        "\n",
        "n_samples, n_features = data.shape\n",
        "n_digits = len(np.unique(y_digits))\n",
        "labels = y_digits\n",
        "\n",
        "sample_size = 300\n",
        "\n",
        "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
        "      % (n_digits, n_samples, n_features))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_digits: 8, \t n_samples 612, \t n_features 76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuOYUVpJEuuD"
      },
      "source": [
        "# Clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to5C51KQDNyO"
      },
      "source": [
        "## Funciones:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y87gPsO0DP3k"
      },
      "source": [
        "### Funci贸n **bench_k_means**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5f2WcLQzcG8"
      },
      "source": [
        "def bench_k_means(estimator, name, data):\n",
        "    t0 = time()\n",
        "    estimator.fit(data)\n",
        "    print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
        "          % (name, (time() - t0), estimator.inertia_,\n",
        "             metrics.homogeneity_score(labels, estimator.labels_),\n",
        "             metrics.completeness_score(labels, estimator.labels_),\n",
        "             metrics.v_measure_score(labels, estimator.labels_),\n",
        "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
        "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
        "             metrics.silhouette_score(data, estimator.labels_,\n",
        "                                      metric='euclidean',\n",
        "                                      sample_size=sample_size), estimator.n_clusters))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce46s8zoETvA"
      },
      "source": [
        "### Funci贸n **bench_DBSCAN**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtDCZ3G0zmn8"
      },
      "source": [
        "def bench_DBSCAN(estimator, name, data, uso_outliers):\n",
        "    t0 = time()\n",
        "    estimator.fit(data)\n",
        "    labels_est = estimator.labels_\n",
        "    labels_originales = labels\n",
        "    if uso_outliers == False:\n",
        "      labels_new = np.where(labels_est==-1, None, labels_est)\n",
        "      # a partir de aqui, experimentar:\n",
        "      index_non_none = [i for i, val in enumerate(labels_new) if val != None]\n",
        "      labels_originales_new = np.zeros(len(index_non_none))\n",
        "      labels_originales_new = [labels_originales[i] for i in index_non_none]\n",
        "      labels_originales = labels_originales_new\n",
        "      labels_est = np.zeros(len(index_non_none))\n",
        "      labels_est = [labels_new[i] for i in index_non_none]\n",
        "      data_new = data[index_non_none]\n",
        "      data = data_new\n",
        "      tamano = len(set(labels_est))\n",
        "      if tamano == 0 or tamano == 1:\n",
        "        print('%-9s\\t%.2fs\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%.3f'\n",
        "              % (name, (time() - t0), 'NaN',\n",
        "                'NaN',\n",
        "                'NaN',\n",
        "                'NaN',\n",
        "                'NaN',\n",
        "                'NaN',\n",
        "                'NaN',\n",
        "                tamano))\n",
        "      else:\n",
        "        #print(labels_est)\n",
        "        print('%-9s\\t%.2fs\\t%s\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
        "              % (name, (time() - t0), 'NaN',\n",
        "                metrics.homogeneity_score(labels_originales, labels_est),\n",
        "                metrics.completeness_score(labels_originales, labels_est),\n",
        "                metrics.v_measure_score(labels_originales, labels_est),\n",
        "                metrics.adjusted_rand_score(labels_originales, labels_est),\n",
        "                metrics.adjusted_mutual_info_score(labels_originales,  labels_est),\n",
        "                metrics.silhouette_score(data, labels_est,\n",
        "                                      metric='euclidean',\n",
        "                                      sample_size=None), tamano))\n",
        "    else:\n",
        "      tamano = len(set(labels_est))\n",
        "      if tamano == 0 or tamano == 1:\n",
        "        print('%-9s\\t%.2fs\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%.3f'\n",
        "              % (name, (time() - t0), 'NaN',\n",
        "                'NaN',\n",
        "                'NaN',\n",
        "                'NaN',\n",
        "                'NaN',\n",
        "                'NaN',\n",
        "                'NaN',\n",
        "                tamano))\n",
        "      else:\n",
        "        #print(labels_est)\n",
        "        print('%-9s\\t%.2fs\\t%s\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
        "              % (name, (time() - t0), 'NaN',\n",
        "                metrics.homogeneity_score(labels_originales, labels_est),\n",
        "                metrics.completeness_score(labels_originales, labels_est),\n",
        "                metrics.v_measure_score(labels_originales, labels_est),\n",
        "                metrics.adjusted_rand_score(labels_originales, labels_est),\n",
        "                metrics.adjusted_mutual_info_score(labels_originales,  labels_est),\n",
        "                metrics.silhouette_score(data, labels_est,\n",
        "                                      metric='euclidean',\n",
        "                                      sample_size=None), tamano))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxReSzFdK_q_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac514e56-8810-42ee-f7bf-5d5488b0e5d5"
      },
      "source": [
        "bench_DBSCAN(DBSCAN(eps = 0.1, min_samples = 5), name=\"dbscan\", data=data, uso_outliers=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dbscan   \t0.03s\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t0.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNhLR6F4FQ4G"
      },
      "source": [
        "## C贸digo principal:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiL6sViSFc4O"
      },
      "source": [
        "## C谩lculo de clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeAdiiXUg3Ja"
      },
      "source": [
        "Hacer pruebas con distintas variantes de algoritmos:\n",
        "* a. K-Means (con inicializaci贸n al azar)\n",
        "* b. K-means++ \n",
        "* c. DBSCAN con 茅psilon por defecto\n",
        "* d. DBSCAN con 茅psilon 0.7\n",
        "* e. DBSCAN con 茅psilon 0.2\n",
        "* f. DBSCAN con 茅psilon por defecto, agregando outliers a cluster extra\n",
        "* g. DBSCAN con 茅psilon 0.7, agregando outliers a cluster extra\n",
        "* h. DBSCAN con 茅psilon 0.2, agregando outliers a cluster extra\n",
        "* i. Clustering aglomerativo\n",
        "* j. Repetir todas las pruebas anteriores, despu茅s de usar PCA sobre los datos para reducirlos a 2 dimensiones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgQPzgATFV73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "63e4683e-3225-4dd6-bc89-0917dfdfdad8"
      },
      "source": [
        "print(90 * '_')\n",
        "print('Sin PCA:')\n",
        "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsil.\\tclusters')\n",
        "## SIN PCA\n",
        "# K-Means random init\n",
        "bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
        "              name=\"random\", data=data)\n",
        "# K-means ++\n",
        "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
        "              name=\"k-means++\", data=data)\n",
        "# DBSCAN, epsilon por defecto\n",
        "bench_DBSCAN(DBSCAN(min_samples = 5), name=\"dbscan_F_df\", data=data, uso_outliers=False)\n",
        "# DBSCAN con eps = 0.7\n",
        "bench_DBSCAN(DBSCAN(eps = 0.7, min_samples = 5), name=\"dbscan_F_0.7\", data=data, uso_outliers=False)\n",
        "# DBSCAN con eps = 0.2\n",
        "bench_DBSCAN(DBSCAN(eps = 0.2, min_samples = 5), name=\"dbscan_F_0.2\", data=data, uso_outliers=False)\n",
        "# DBSCAN, epsilon por defecto, outliers = True\n",
        "bench_DBSCAN(DBSCAN(min_samples = 5), name=\"dbscan_T_def\", data=data, uso_outliers=True)\n",
        "# DBSCAN con eps = 0.7, outliers = True\n",
        "bench_DBSCAN(DBSCAN(eps = 0.7, min_samples = 5), name=\"dbscan_T_0.7\", data=data, uso_outliers=True)\n",
        "# DBSCAN con eps = 0.2, outliers = True\n",
        "bench_DBSCAN(DBSCAN(eps = 0.2, min_samples = 5), name=\"dbscan_T_0.2\", data=data, uso_outliers=True)\n",
        "# Clustering Aglomerativo\n",
        "bench_DBSCAN(AgglomerativeClustering(n_clusters=n_digits,linkage=\"ward\"), name=\"Agglo\", data=data, uso_outliers=True)\n",
        "##############################################################################\n",
        "print('')\n",
        "## CON PCA\n",
        "print('Con PCA:')\n",
        "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsil.\\tclusters')\n",
        "pca = PCA(n_components=2).fit(data)\n",
        "data_pca = pca.transform(data)\n",
        "# K-Means random init\n",
        "bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
        "              name=\"random_PCA\", data=data_pca)\n",
        "# K-means ++\n",
        "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
        "              name=\"k-means++_PCA\", data=data_pca)\n",
        "# DBSCAN, epsilon por defecto\n",
        "bench_DBSCAN(DBSCAN(min_samples = 5), name=\"dbscan_F_df\", data=data_pca, uso_outliers=False)\n",
        "# DBSCAN con eps = 0.7\n",
        "bench_DBSCAN(DBSCAN(eps = 0.7, min_samples = 5), name=\"dbscan_F_0.7\", data=data_pca, uso_outliers=False)\n",
        "# DBSCAN con eps = 0.2\n",
        "bench_DBSCAN(DBSCAN(eps = 0.2, min_samples = 5), name=\"dbscan_F_0.2\", data=data_pca, uso_outliers=False)\n",
        "# DBSCAN, epsilon por defecto, outliers = True\n",
        "bench_DBSCAN(DBSCAN(min_samples = 5), name=\"dbscan_T_df_P\", data=data_pca, uso_outliers=True)\n",
        "# DBSCAN con eps = 0.7, outliers = True\n",
        "bench_DBSCAN(DBSCAN(eps = 0.7, min_samples = 5), name=\"DBSCAN_T_0.7\", data=data_pca, uso_outliers=True)\n",
        "# DBSCAN con eps = 0.2, outliers = True\n",
        "bench_DBSCAN(DBSCAN(eps = 0.2, min_samples = 5), name=\"DBSCAN_T_0.2\", data=data_pca, uso_outliers=True)\n",
        "# Clustering Aglomerativo\n",
        "bench_DBSCAN(AgglomerativeClustering(n_clusters=n_digits,linkage=\"ward\"), name=\"Agglo\", data=data_pca, uso_outliers=True)\n",
        "print(90 * '_')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________\n",
            "Sin PCA:\n",
            "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsil.\tclusters\n",
            "random   \t0.10s\t1125\t0.296\t0.298\t0.297\t0.140\t0.283\t0.208\t8.000\n",
            "k-means++\t0.18s\t1122\t0.305\t0.306\t0.305\t0.158\t0.291\t0.222\t8.000\n",
            "dbscan_F_df\t0.05s\tNaN\t1.000\t0.705\t0.827\t0.539\t0.771\t0.693\t12.000\n",
            "dbscan_F_0.7\t0.06s\tNaN\t0.975\t0.536\t0.692\t0.263\t0.630\t0.373\t43.000\n",
            "dbscan_F_0.2\t0.04s\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t0.000\n",
            "dbscan_T_def\t0.05s\tNaN\t0.107\t0.365\t0.165\t0.002\t0.113\t-0.349\t13.000\n",
            "dbscan_T_0.7\t0.06s\tNaN\t0.579\t0.425\t0.490\t0.072\t0.422\t-0.006\t44.000\n",
            "dbscan_T_0.2\t0.04s\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t1.000\n",
            "Agglo    \t0.02s\tNaN\t0.352\t0.356\t0.354\t0.168\t0.341\t0.159\t8.000\n",
            "\n",
            "Con PCA:\n",
            "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsil.\tclusters\n",
            "random_PCA\t0.08s\t346\t0.214\t0.217\t0.215\t0.099\t0.199\t0.363\t8.000\n",
            "k-means++_PCA\t0.11s\t345\t0.215\t0.217\t0.216\t0.098\t0.200\t0.345\t8.000\n",
            "dbscan_F_df\t0.01s\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t1.000\n",
            "dbscan_F_0.7\t0.01s\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t1.000\n",
            "dbscan_F_0.2\t0.00s\tNaN\t0.284\t0.283\t0.284\t0.068\t0.212\t0.083\t20.000\n",
            "dbscan_T_df_P\t0.01s\tNaN\t0.006\t0.116\t0.011\t0.000\t0.005\t0.374\t2.000\n",
            "DBSCAN_T_0.7\t0.01s\tNaN\t0.005\t0.335\t0.010\t0.000\t0.005\t0.505\t2.000\n",
            "DBSCAN_T_0.2\t0.01s\tNaN\t0.195\t0.202\t0.198\t0.037\t0.143\t-0.205\t21.000\n",
            "Agglo    \t0.01s\tNaN\t0.219\t0.226\t0.223\t0.102\t0.206\t0.354\t8.000\n",
            "__________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}